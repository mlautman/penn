%run ../ML/Tournament.py -h
Usage: usage: Tournament.py  [options]

Options:
  -h, --help            show this help message and exit
  -v, --verbose         Enable verbose output. Note that this setting takes
                        advantage of a per-process runtime setting in libsvm
                        that, if enabled, may not work properly in a
                        multithreaded context.

  File I/O options:
    file system options

    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output path for dump files
    -n FNAME, --name=FNAME
    --save_cm           Assign --save_cm if you want to save the confusion
                        matrix to both a csv and an image
    --save_roc          Assign --save_roc if you want to save the ROC curve as
                        an image

  Algorithm Options::
    Choosing the right algorithm is really important in life.

    -a LEARNING_ALGORITHM, --algorithm=LEARNING_ALGORITHM
                        Select learning algorithm. one of: svm, logreg, boost,
                        dt, knn
    -k FOLDS, --k_folds=FOLDS
                        number of folds for K-folds
    -f INTERPOLATION_SIZE, --interpolation=INTERPOLATION_SIZE
                        Standardized lenght of recorded signal
    -c CV_SPLITS, --cv_splits=CV_SPLITS
                        The number of cross validation splits

  SVM.svc specific options:
    options for SVM svc classifiers

    --svc_C=C           Value of C: Inverse of regularization strength; must
                        be a positive float. Like in support vector machines,
                        smaller values specify stronger regularization.
    --kernel=KERNEL     kernel for SVC (rbf, sigmoid, precomputed, poly,
                        linear)
    --degree=DEGREE     optional (default=3) Degree of the polynomial kernel
                        function ('poly'). Ignored by all other kernels.
    --max_iter=MAX_ITER
                        max_iterations before halting
    --gamma=GAMMA       Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. If
                        gamma is 0.0 then 1/n_features will be used instead.
    --tolerance=TOL     tolerance for halting

  Logistic Regression specific options:
    --penalty=PENALTY   Used to specify the norm used in the penalization.
                        {l1, l2}
    --C_lr=C            Value of C: Inverse of regularization strength; must
                        be a positive float. Like in support vector machines,
                        smaller values specify stronger regularization.
    --tolerance_lr=TOL  tolerance for halting

  AdaBoost specific options:
    NOTE: AdaBoost will also use options from DecisionTreeClassifier
    options if base_estimator = DecisionTreeClassifier

    --base_estimator=BASE_ESTIMATOR
                        AdaBoost only The base estimator from which the
                        boosted ensemble is built. Support for sample
                        weighting is required, as well as proper classes_ and
                        n_classes_ attributes.  {eg.DecisionTreeClassifier}
    --n_estimators=N_ESTIMATORS
                        The maximum number of estimators at which boosting is
                        terminated. In case of perfect fit, the learning
                        procedure is stopped early.(default=50)
    --learning_rate=LEARNING_RATE
                        Learning rate shrinks the contribution of each
                        classifier by learning_rate. There is a trade-off
                        between learning_rate and n_estimators.

  DecisionTreeClassifier specific options:
    NOTE: DecisionTreeClassifier options will also affect
    ADABoostClassifier if ADABoostClassifier is selected and
    base_estimator = DecisionTreeClassifier

    --splitter=SPLITTER
                        The strategy used to choose the split at each node.
                        Supported strategies are 'best' to choose the best
                        split and 'random' to choose the best random split.
    --max_features=MAX_FEATURES
                        number of features explored           int: only look
                        at max_features features           float: look at
                        percent of n_features           auto: n_features
                        sqrt: look at sqrt(n_features)          log2: look at
                        log2(n_features)
    --max_depth=MAX_DEPTH
                        The maximum depth of the tree. If None, then nodes are
                        expanded until all leaves are pure or until all leaves
                        contain less than min_samples_split samples. Ignored
                        if max_samples_leaf is not None.
    --min_samples_split=MIN_SAMPLES_SPLIT
                        The minimum number of samples required to split an
                        internal node.
    --min_samples_leaf=MIN_SAMPLES_LEAF
                        The minimum number of samples required to be at a leaf
                        node.
    --max_leaf_nodes=MAX_LEAF_NODES
                        Grow a tree with max_leaf_nodes in best-first fashion.
                        Best nodes are defined as relative reduction in
                        impurity. If None then unlimited number of leaf nodes.
                        If not None then max_depth will be ignored.

  KNN DTW specific options (NOT IMPLEMENTED!!):
    --n_neighbors=N_NEIGHBORS
                        int, optional (default=5) Number of neighbors to use
                        by default for KNN
    --max_warping_window=MAX_WARPING_WINDOW
                        int, optional (default = 1000000) Maximum warping
                        window allowed by the DTW dynamic programming function
    --subsample_step=SUBSAMPLE_STEP
                        int, optional (default = 1) Step size for the
                        timeseries array. By setting subsample_step = 2, the
                        timeseries length will be reduced by half because
                        every second item is skipped. Implemented by x[:,
                        ::subsample_step]